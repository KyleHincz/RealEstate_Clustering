{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "\n",
    "import matplotlib.pyplot as plt # For plots\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appr_full = pd.read_pickle(\"./IN_dfs/df_appr_full_raw.pkl\")\n",
    "df_comp_full = pd.read_pickle(\"./IN_dfs/df_comp_full_raw.pkl\")\n",
    "print(\"Appriasal Shape\", df_appr_full.shape)\n",
    "print(\"Comp Shape\", df_comp_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After changing which df to process Run all cells below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select which dataframe to process. Comment out the df you don't need. Run the entire notebook twice to save both dataframes.\n",
    "\n",
    "#which_df = \"APPRAISAL\"\n",
    "which_df = \"COMPARABLES\"\n",
    "\n",
    "show_analyse_column_output = False # False surpresses output and speeds out running of the notebook\n",
    "\n",
    "if which_df == \"APPRAISAL\":\n",
    "    df_used = df_appr_full.copy()\n",
    "elif which_df == \"COMPARABLES\":\n",
    "    df_used = df_comp_full.copy()\n",
    "    cols = df_appr_full.columns\n",
    "    df_used.columns = cols\n",
    "else:\n",
    "    raise ValueError(\"############Incorrect dataframe selected#############\")\n",
    "    \n",
    "df_new_used = pd.DataFrame() # new dataframe post data cleaning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for c, a in zip(df_comp_full.columns, df_appr_full.columns):\n",
    "    print(c,\"///\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create some smaller dataframes\n",
    "# using the train test split function\n",
    "#Y_90pc, Y_10pc = train_test_split(df_appr_full,\n",
    "#                                   random_state=104, \n",
    "#                                   test_size=0.1, \n",
    "#                                   shuffle=True)\n",
    "#Y_6m_10k, Y_10k = train_test_split(df_appr_full,\n",
    "#                                   random_state=104, \n",
    "#                                   test_size=10000, \n",
    "#                                   shuffle=True)\n",
    "\n",
    "#Y_6m_100k, Y_100k = train_test_split(df_appr_full,\n",
    "#                                   random_state=104, \n",
    "#                                   test_size=100000, \n",
    "#                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize each column for batch processing and wite helper funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_binary = ['LOCRTGNEUTRAL_SUBJ', 'LOCRTGBENEFICIAL_SUBJ',\n",
    "       'LOCRTGADVERSE_SUBJ', 'LOCRESIDENTIAL_SUBJ', 'LOCINDUSTRIAL_SUBJ',\n",
    "       'LOCCOMMERCIAL_SUBJ', 'LOCBUSYROAD_SUBJ', 'LOCWATERFRONT_SUBJ',\n",
    "       'LOCGOLFCOURSE_SUBJ', 'LOCADJPARK_SUBJ', 'LOCADJPOWERLINES_SUBJ',\n",
    "       'LOCLANDFILL_SUBJ', 'LOCPUBLICTRANS_SUBJ','VIEWRTGNEUTRAL_SUBJ', 'VIEWRTGBENEFICIAL_SUBJ', 'VIEWRTGADVERSE_SUBJ',\n",
    "       'VIEWTYPEWATER_SUBJ', 'VIEWTYPEPASTORAL_SUBJ', 'VIEWTYPEWOODS_SUBJ',\n",
    "       'VIEWTYPEPARK_SUBJ', 'VIEWTYPEGOLFCOURSE_SUBJ',\n",
    "       'VIEWTYPECITYSKYLINE_SUBJ', 'VIEWTYPEMOUNTAIN_SUBJ',\n",
    "       'VIEWTYPERESIDENTIAL_SUBJ', 'VIEWTYPECITYSTREET_SUBJ',\n",
    "       'VIEWTYPEINDUSTRIAL_SUBJ', 'VIEWTYPEPOWERLINES_SUBJ',\n",
    "       'VIEWTYPELIMITED_SUBJ' ] # convert to 0/1 columns\n",
    "columns_quality = ['QUALITYOFCONSTQ1_SUBJ','QUALITYOFCONSTQ2_SUBJ', 'QUALITYOFCONSTQ3_SUBJ','QUALITYOFCONSTQ4_SUBJ', \n",
    "                   'QUALITYOFCONSTQ5_SUBJ', 'QUALITYOFCONSTQ6_SUBJ'] # convert to 1 continuous column?\n",
    "columns_condition = ['CONDITIONC1_SUBJ','CONDITIONC2_SUBJ', 'CONDITIONC3_SUBJ', 'CONDITIONC4_SUBJ',\n",
    "                       'CONDITIONC5_SUBJ', 'CONDITIONC6_SUBJ'] # convert to 1 continuous column?\n",
    "columns_numeric = ['TOTALRMS_SUBJ', 'BDRMS_SUBJ',\n",
    "        'BLGRDTOTALSQFT_SUBJ', 'BLGRDFINISHSQFT_SUBJ',\n",
    "       'BLGRDRECRMS_SUBJ', 'BLGRDBEDRMS_SUBJ', \n",
    "       'BLGRDOTHERRMS_SUBJ', 'GROSSLIVINGAREA_SUBJ'] \n",
    "columns_age = ['ACTUALAGE_SUBJ']\n",
    "columns_bathrooms = ['BATHS_SUBJ','BLGRDBATHRMS_SUBJ'] #split into two\n",
    "\n",
    "### Columns below need most attention ###\n",
    "date_columns = [  'SALEDATE_SUBJ', 'COMPSALEDATE'] # almost doen - need year\n",
    "site_columns = ['SITE_SUBJ']\n",
    "\n",
    "special_columns = [ 'DESIGNSTYLE_SUBJ', 'HEATINGCOOLING_SUBJ', 'ENERGYEFF_SUBJ',\n",
    "       'GARAGECARPORT_SUBJ', 'PORCHPATIODECK_SUBJ' ]\n",
    "\n",
    "lat_long_columns = ['APPRLATITUDE_SUBJ', 'APPRLONGITUDE_SUBJ',]\n",
    "\n",
    "identifier_columns = ['SUBJ_APPR_ID','COMPNUM', 'ADDRESS1_SUBJ', 'CITY_SUBJ', 'STATE_SUBJ',\n",
    "       'ZIPCODE_SUBJ', 'COUNTY_SUBJ']\n",
    "\n",
    "ignored_columns = ['LOCATIONDESC_SUBJ', 'VIEWDESC_SUBJ','BASEMENT_SUBJ','FINISHEDRMSBLWGRD_SUBJ']\n",
    "# locationdesc, site and viewdesc already encoded into binary columns in data, can ignore them\n",
    "# 'BASEMENT_SUBJ','FINISHEDRMSBLWGRD_SUBJ' also already ensoded into columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the individual lists above have all the columns from main dataframe\n",
    "columns_being_analysed = [ *identifier_columns, *columns_binary,*columns_quality, *columns_condition, *columns_numeric, *site_columns,\n",
    "                                *columns_bathrooms, *special_columns, *date_columns, *ignored_columns,\n",
    "                                *lat_long_columns, *columns_age]\n",
    "\n",
    "missing_cols = [item for item in list(df_appr_full.columns) \n",
    "                if item not in columns_being_analysed ]\n",
    "\n",
    "if len(missing_cols)== 0:\n",
    "    print(\"No missing columns\")\n",
    "else:\n",
    "     print(\"#### THERE ARE MISSING COLUMNS ####\\n\",missing_cols)   \n",
    "\n",
    "duplicates = [i for i in columns_being_analysed if columns_being_analysed.count(i) > 1]\n",
    "unique_duplicates = list(set(duplicates))\n",
    "print(\"\\nDuplicate columns:\\n\\n\", unique_duplicates)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_name_adjuster(columns_list, source_df):\n",
    "    # takes in a list of columns as strings and source df and figures out if the columns are in it.\n",
    "    # returns column list with names either with or without _SUBJ. Agnostic of whether using appr or comp frames\n",
    "    if all(item in source_df.columns for item in columns_list):\n",
    "        return (columns_list)\n",
    "    else:\n",
    "        if \"_SUBJ\" in columns_list[0]:\n",
    "            columns_list1 = [elem[:-5] for elem in columns_list]\n",
    "            return (columns_list1)\n",
    "        else:\n",
    "            columns_list2 = [elem + \"_SUBJ\" for elem in columns_list]\n",
    "            return (columns_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_summary(col,dataframe):\n",
    "    # column summary - pass col name as string + dataframe. Receive sorted occurence count and %\n",
    "    # TODO nan handling - atm not showing\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    counts = dataframe[col].value_counts()\n",
    "    percs = dataframe[col].value_counts(normalize=True).mul(100).round(3).astype(str) + '%'\n",
    "    return pd.concat([counts,percs], axis=1, keys=['count', 'percentage'])\n",
    "\n",
    "\n",
    "def analyse_column_list(col_lst, df,show=True):\n",
    "    if show == False:\n",
    "        return None\n",
    "    else:\n",
    "        col_lst1 = column_name_adjuster(col_lst, df)\n",
    "        for column in col_lst1:\n",
    "            print(\"\\n*********\",'\\033[1m' + column + '\\033[0m')\n",
    "            display(column_summary(column,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform identifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp[identifier_columns] = df_used[identifier_columns].replace({'[NULL]': np.nan})\n",
    "#print(identifier_columns)\n",
    "temp['SUBJ_APPR_ID'] = temp['SUBJ_APPR_ID'].astype(\"Int64\")\n",
    "temp['COMPNUM'] = temp['COMPNUM'].astype(\"Int64\")\n",
    "analyse_column_list(identifier_columns,temp.head(1000),show=False)\n",
    "\n",
    "df_new_used[identifier_columns] = temp[identifier_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform binary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all binary columns \n",
    "analyse_column_list(columns_binary, df_used,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all quality columns # might want to convert these to continuous #TODO?\n",
    "analyse_column_list(columns_quality, df_used,show=False)\n",
    "analyse_column_list(columns_condition, df_used,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all binary columns\n",
    "\n",
    "df_new_used[columns_binary] = df_used[columns_binary].replace({'Y': 1,'[NULL]': 0})\n",
    "df_new_used[columns_quality] = df_used[columns_quality].replace({'Y': 1,'[NULL]': 0})\n",
    "df_new_used[columns_condition] = df_used[columns_condition].replace({'Y': 1,'[NULL]': 0})\n",
    "\n",
    "analyse_column_list([*columns_binary,*columns_quality,*columns_condition],df_new_used,show=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp4 = pd.DataFrame()\n",
    "for j in lat_long_columns:\n",
    "    df_temp4[j]=pd.to_numeric(df_used[j],errors=\"ignore\",downcast=\"float\")\n",
    "    df_temp4[j] = df_temp4[j].replace([0.,\"[NULL]\"], np.nan)\n",
    "    \n",
    "df_new_used[lat_long_columns] = df_temp4[lat_long_columns]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform numeric columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all numeric columns PRoposing to replace all NULLs with zero TODO\n",
    "analyse_column_list(columns_numeric,df_used,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean numeric cols\n",
    "for num_col in columns_numeric:\n",
    "    df_new_used[num_col] = df_used[num_col].str.replace(r'[^\\d.]+', '', regex=True)\n",
    "    df_new_used[num_col] = df_new_used[num_col].str.replace(r'[^\\d.]+', '', regex=True) #remove character if not decimal digit or .\n",
    "    df_new_used[num_col] = df_new_used[num_col].str.replace(r'^(0*)', '',regex=True) #remove leading zero\n",
    "    #df_new_used[df_new_used[\"TOTALRMS_SUBJ\"]==\"\"] = np.NaN\n",
    "    df_new_used[num_col]=pd.to_numeric(df_new_used[num_col],errors=\"ignore\",downcast=\"integer\")\n",
    "    #TODO fill NA\n",
    "    #df_new_used['TOTALRMS_SUBJ'] = df_new_used['TOTALRMS_SUBJ'].astype(int)\n",
    "\n",
    "analyse_column_list(columns_numeric,df_new_used,show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Check date columns\n",
    "df_temp=pd.DataFrame()\n",
    "df_temp[date_columns] = df_used[date_columns]\n",
    "\n",
    "#clean up the extracted_number field\n",
    "for c in date_columns:\n",
    "    if c == \"SALEDATE_SUBJ\":\n",
    "        lst = df_temp[c].unique()\n",
    "        print(c, \"length\", len(lst))\n",
    "        lst_num = []\n",
    "        for no in lst:\n",
    "            try:\n",
    "                #print(no)\n",
    "                test = datetime.datetime.strptime(no, '%m/%d/%Y')\n",
    "                lst_num.append(test)\n",
    "                #print(test)\n",
    "            except Exception as e: \n",
    "                if no==\"[NULL]\":\n",
    "                    lst_num.append(np.nan)\n",
    "                else:\n",
    "                    print(\"error\",no)\n",
    "        df_new_used[c] = df_temp[c]\n",
    "    if c == \"COMPSALEDATE\":\n",
    "        lst = df_temp[c].unique()\n",
    "        print(c, \"length\", len(lst))\n",
    "        lst_num = []\n",
    "        for no in lst:\n",
    "            try:\n",
    "                #print(no)\n",
    "                test = datetime.datetime.strptime(no, '%m/%y')\n",
    "                lst_num.append(test)\n",
    "                #print(test)\n",
    "            except Exception as e: \n",
    "                try:\n",
    "                    #print(no)\n",
    "                    test = datetime.datetime.strptime(no, '%m/%d/%Y')\n",
    "                    lst_num.append(test)\n",
    "                    #print(test)\n",
    "                except Exception as e:\n",
    "                    if no==\"[NULL]\":\n",
    "                        lst_num.append(np.nan)\n",
    "                    else:\n",
    "                        lst_num.append(np.nan)\n",
    "                        print(\"replaced with NaN:\",no)\n",
    "    date_dict = dict(zip(lst, lst_num))\n",
    "   # df_new_used[c] = df_used[c].replace(date_dict)\n",
    "    df_new_used[c] = df_used[c].map(date_dict.get)\n",
    "    \n",
    "df_new_used[\"SALEDATE_SUBJ\"] = pd.to_datetime(df_new_used[\"SALEDATE_SUBJ\"], errors = 'coerce')\n",
    "df_new_used[\"COMPSALEDATE\"] = pd.to_datetime(df_new_used[\"COMPSALEDATE\"], errors = 'coerce')\n",
    "\n",
    "print(\"######Date columns transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_used.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up age\n",
    "analyse_column_list(columns_age,df_used,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up ACTUALAGE_SUBJ #TODO if age is > 1900, deduct year of appraisal to get age. Clarify age>>100\n",
    "\n",
    "lst = df_used['ACTUALAGE_SUBJ'].unique()\n",
    "\n",
    "import re\n",
    "lst_num = []\n",
    "for no in lst:\n",
    "        temp_list = re.findall(r'\\d+', no)\n",
    "        \n",
    "        if len(temp_list)==0:\n",
    "            x =(np.NaN)\n",
    "        elif len(temp_list)==1:\n",
    "            x =(temp_list[0])\n",
    "        else:\n",
    "            x = (temp_list[-1])\n",
    "        \n",
    "        if pd.isnull(x):\n",
    "            lst_num.append(x)\n",
    "        \n",
    "        elif len(x) >1:\n",
    "            x = x.lstrip('0')\n",
    "            if x ==\"\":\n",
    "                x='0'\n",
    "            lst_num.append(int(x))\n",
    "        else:\n",
    "            lst_num.append(int(x))\n",
    "age_dict = dict(zip(lst, lst_num))\n",
    "\n",
    "##########df_new_used[columns_age] = df_used[columns_age].replace(age_dict) # REACTIVATE\n",
    "analyse_column_list(columns_age,df_new_used,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_temp_2 = pd.DataFrame()\n",
    "df_temp_2['ACTUALAGE_SUBJ_original'] = df_used['ACTUALAGE_SUBJ'].map(age_dict.get)\n",
    "df_temp_2[date_columns] = df_new_used[date_columns].copy()\n",
    "#print(df_temp_2.head())\n",
    "counter = df_temp_2.count()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if len(list(counter)) == 1:\n",
    "    print(\"a\")\n",
    "    date_column = date_columns[0]\n",
    "else:\n",
    "    len_check = (list(counter)[1]) > (list(counter)[0])\n",
    "    if len_check:\n",
    "        print(\"b\")\n",
    "        date_column = date_columns[0]\n",
    "    else:\n",
    "        print(\"c\")\n",
    "        date_column = date_columns[1]\n",
    "print(date_column)\n",
    "#print(df_temp_2.head(20))\n",
    "\n",
    "# do the transformations\n",
    "#df_temp_2[\"Year\"] = df_temp_2[date_column].dt.year\n",
    "\n",
    "df_temp_2[\"Year\"] = pd.DatetimeIndex(df_temp_2[date_column]).year\n",
    "\n",
    "\n",
    "df_temp_2.loc[df_temp_2[\"ACTUALAGE_SUBJ_original\"] > 1000.,\"col2\" ] = df_temp_2['Year']- df_temp_2[\"ACTUALAGE_SUBJ_original\"]\n",
    "df_temp_2[\"col3\"] = df_temp_2[\"col2\"].fillna(df_temp_2[\"ACTUALAGE_SUBJ_original\"])\n",
    "df_temp_2['col4'] = df_temp_2['col3'].where((0 <= df_temp_2['col3']) & (df_temp_2['col3'] <= 250)) # limit age to 0-250\n",
    "\n",
    "df_new_used[\"ACTUALAGE_SUBJ\"] = df_temp_2['col4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform bathroom columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up 'BATHS_SUBJ', 'BLGRDBATHRMS_SUBJ'\n",
    "def clean_bathrooms(columns_list, source_df,which_col):\n",
    "    # gets list of column names with bathrooms and source df\n",
    "    # returns 1 full bathroom and 1 half bathroom column for each input column\n",
    "    # specify \"full\" or \"half\" in which_col to select whihc column to transform\n",
    "    cols_to_use = column_name_adjuster(columns_list, source_df)\n",
    "    lst = np.unique(source_df[cols_to_use].values)\n",
    "\n",
    "    # clean up and convert all combinations of baths into float\n",
    "    import re\n",
    "    lst_num = []\n",
    "    for no in lst:\n",
    "            temp_list = re.findall(r'[\\d\\.]+', no)\n",
    "\n",
    "            if len(temp_list)==0:\n",
    "                x =(np.NaN)\n",
    "            else: \n",
    "                x =(temp_list[0])\n",
    "            lst_num.append(float(x))\n",
    "    #print(lst_num)\n",
    "\n",
    "    # Deal with half baths\n",
    "    half_bath_list =[]\n",
    "    for string in lst_num:\n",
    "        x = round(string % 1,2)\n",
    "        if pd.isnull(x):\n",
    "            half_bath_list.append(x)\n",
    "        elif x==0:\n",
    "            half_bath_list.append(0)\n",
    "        elif 0 < x < 0.2:\n",
    "            half_bath_list.append(1)\n",
    "        elif 0.2 <=  x < 0.3:\n",
    "            half_bath_list.append(2)\n",
    "        elif 0.3 <= x < 0.4:\n",
    "            half_bath_list.append(3)\n",
    "        elif 0.4 <= x < 0.5:\n",
    "            half_bath_list.append(4)\n",
    "        else:\n",
    "            half_bath_list.append(1) # anything above 0.5 is assumed to be 1 half bath\n",
    "\n",
    "        #print(string , round((string % 1.0),2), half_bath_list[-1])  \n",
    "    #print(half_bath_list)\n",
    "\n",
    "    #deal with full baths\n",
    "    full_bath_list =[]\n",
    "    for string in lst_num:\n",
    "\n",
    "        if pd.isnull(string):\n",
    "            full_bath_list.append(string)\n",
    "        else:\n",
    "            full_bath_list.append(int(string))\n",
    "        #print(string ,  full_bath_list[-1])  \n",
    "    #print(full_bath_list)\n",
    "\n",
    "    full_bath_dict = dict(zip(lst, full_bath_list))\n",
    "    half_bath_dict = dict(zip(lst, half_bath_list))\n",
    "    #print(full_bath_dict)\n",
    "    # create new columns\n",
    "    df_temp = source_df[cols_to_use]\n",
    "    \n",
    "    if which_col == \"full\":\n",
    "        for j in cols_to_use:\n",
    "            df_temp[\"FULL_\"+j] = source_df[j].map(full_bath_dict.get)\n",
    "            #df_temp[\"HALF_\"+j] = source_df[j].map(half_bath_dict.get)\n",
    "        #print(df_temp.head(25))\n",
    "        #print(df_temp.columns.str.contains(r'full', case=False))\n",
    "        #print(df_temp[df_temp.columns[df_temp.columns.str.contains(r'full', case=False)]].head())\n",
    "        return (df_temp[df_temp.columns[df_temp.columns.str.contains(r'full', case=False)]])\n",
    "    \n",
    "    elif which_col == \"half\":\n",
    "        for j in cols_to_use:\n",
    "            #df_temp[\"FULL_\"+j] = source_df[j].map(full_bath_dict.get)\n",
    "            df_temp[\"HALF_\"+j] = source_df[j].map(half_bath_dict.get)\n",
    "        #print(df_temp.head(25))\n",
    "        #print(df_temp.columns.str.contains(r'half', case=False))\n",
    "        #print(df_temp[df_temp.columns[df_temp.columns.str.contains(r'half', case=False)]].head())\n",
    "        return (df_temp[df_temp.columns[df_temp.columns.str.contains(r'half', case=False)]])\n",
    "    else:\n",
    "        print(\"Full or Half not specified!! Exited\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add new bathroom columns to used dataframe \n",
    "\n",
    "zzz_full = clean_bathrooms(['BATHS', 'BLGRDBATHRMS'],df_used,\"full\")\n",
    "df_new_used[zzz_full.columns] = zzz_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz_half = clean_bathrooms(['BATHS', 'BLGRDBATHRMS'],df_used,\"half\")\n",
    "df_new_used[zzz_half.columns] = zzz_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and transform site (area sqft/ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=pd.DataFrame()\n",
    "df_temp[\"SITE_SUBJ\"] = df_used[site_columns[0]]\n",
    "df_temp[\"no_comma\"] = df_temp[\"SITE_SUBJ\"].str.replace(',', '')\n",
    "df_temp['extracted_Number'] = df_temp['no_comma'].str.replace('([A-Za-z]+)', '',regex=True)\n",
    "df_temp['extracted_Letter'] = df_temp['no_comma'].str.extract('([A-Za-z]+)')\n",
    "df_temp['extracted_Letter'] = df_temp['extracted_Letter'].str.lower()\n",
    "\n",
    "#clean up the extracted_number field\n",
    "\n",
    "lst = df_temp['extracted_Number'].unique()\n",
    "#print(len(lst))\n",
    "import re\n",
    "lst_num = []\n",
    "for no in lst:\n",
    "    \n",
    "    try:\n",
    "        test = float(no)\n",
    "        lst_num.append(test)\n",
    "    except Exception as e: \n",
    "        #print(no)\n",
    "        #x = no.lstrip()\n",
    "        x = no.lstrip('0')\n",
    "        #print(x)\n",
    "        temp_list = re.findall(r'[+-]?([1-9]\\d*(\\.\\d*[1-9])?|0\\.\\d*[1-9]+)|\\d+(\\.\\d*[1-9])?', x)#look for numbers\n",
    "        if len(temp_list)==0:\n",
    "            lst_num.append(np.nan)\n",
    "        else:\n",
    "            #print(temp_list)\n",
    "            try:\n",
    "                lst_num.append(float(temp_list[0][0]))#take first number\n",
    "            except Exception as e: \n",
    "                lst_num.append(float(temp_list[1][0]))#take first number\n",
    "\n",
    "site_dict = dict(zip(lst, lst_num))\n",
    "\n",
    "df_temp[\"extracted_Number_clean\"] = df_temp[\"extracted_Number\"].map(site_dict.get) #QUICKEST!!!!\n",
    "#analyse_column_list([\"extracted_Number_clean\"],df_temp)\n",
    "\n",
    "# Clean up number and Convert ac to sqft \n",
    "df_temp['extracted_Letter_with_nan'] = df_temp['extracted_Letter']\n",
    "area_unit_dict = {\"sf\": 1., \"s\": 1., \"sq\": 1., \"sqft\": 1., \"ac\": 43560.}\n",
    "df_temp['conversion_rate'] = df_temp['extracted_Letter'].map(area_unit_dict).fillna(1.0) # assuming any populated areas without unit are sqft\n",
    "df_temp['SITEAREASQFT'] = df_temp['extracted_Number_clean'].mul(df_temp['conversion_rate'])\n",
    "\n",
    "# Add \"SITEAREASQFT\" to df_used\n",
    "df_new_used['SITEAREASQFT_SUBJ'] = df_temp['SITEAREASQFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply other last minute transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA in Belowground\n",
    "\n",
    "bldgrd = df_new_used.columns[df_new_used.columns.str.contains(r'blgrd', case=False)]\n",
    "df_new_used[bldgrd] = df_new_used[bldgrd].fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove _SUBJ suffix\n",
    "\n",
    "df_new_used.columns = df_new_used.columns.str.rstrip('_SUBJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new_used.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processsed dataframes to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if which_df == \"APPRAISAL\":\n",
    "    df_new_used.to_pickle(\"./OUT_dfs/df_appr_full_processed.pkl\")\n",
    "    print(\"!!!!!!!!!!!!!APPRAISAL SAVED!!!!!!!!!!!!!!!\")\n",
    "elif which_df == \"COMPARABLES\":\n",
    "    df_new_used.to_pickle(\"./OUT_dfs/df_comp_full_processed.pkl\") # column names have been changed to be same as in APPR df\n",
    "    print(\"!!!!!!!!!!!!!COMP SAVED!!!!!!!!!!!!!!!\")\n",
    "else:\n",
    "    raise ValueError(\"############Incorrect dataframe selected#############\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
