{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42142a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt, cos, sin, asin, radians\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fae05",
   "metadata": {},
   "source": [
    "# Load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f095e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets:\n",
    "df_appr_nona = pd.read_pickle(\"./OUT_dfs/df_appr_full_processed_nona.pkl\")\n",
    "df_comp_unique = pd.read_pickle(\"./OUT_dfs/uniqueComps.pkl\")\n",
    "\n",
    "# Dictionary of appraisal\n",
    "dict_apprid_to_uniquecompidnew = pickle.load(open(\"./OUT_dfs/dict_apprid_to_uniquecompidnew\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_appr = 856775\n",
    "dict_apprid_to_uniquecompidnew[test_appr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81576e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of loadings:\n",
    "appr = df_appr_nona.copy()\n",
    "comp = df_comp_unique.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa62a9e",
   "metadata": {},
   "source": [
    "# Drop rows that have null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaeede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to drop any NaN in the defined columns:\n",
    "\n",
    "def remove_nan(a_df, c_df):\n",
    "    a_cols = ['APPRLONGITUDE', 'APPRLATITUDE', 'SALEDATE']\n",
    "    c_cols = ['APPRLONGITUDE', 'APPRLATITUDE', 'SALEDATE'] # COMPSALEDATE\n",
    "    a_full = a_df.dropna(subset=a_cols)\n",
    "    c_full = c_df.dropna(subset=c_cols)\n",
    "    \n",
    "    return a_full, c_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "appr_no_nan, comp_no_nan = remove_nan(appr, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebad041",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_no_nan.loc[comp_no_nan['UNIQUECOMPIDNEW'].isin(dict_apprid_to_uniquecompidnew[856775])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea45f7",
   "metadata": {},
   "source": [
    "# Dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def full_or_sample_dataset(a_df, c_df, sub_sample=None, state=None):\n",
    "#     if sub_sample == None and state == None:\n",
    "#         a_train = a_df.copy() # computationally expensive\n",
    "#         c_train = c_df[c_df['SUBJ_APPR_ID'].isin(a_train['SUBJ_APPR_ID'])]\n",
    "#         print('Full dataset loaded successfully!')\n",
    "#     elif sub_sample == None and state!=None:\n",
    "#         a_train = a_df[a_df['STATE'] == county]\n",
    "#         c_train = c_df[c_df['SUBJ_APPR_ID'].isin(a_train['SUBJ_APPR_ID'])]\n",
    "#         print('Subset from state ' + state + ' loaded successfully!')\n",
    "#     elif sub_sample != None and state == None:\n",
    "#         a_train = a_df.sample(n=sub_sample)\n",
    "#         c_train = c_df[c_df['SUBJ_APPR_ID'].isin(a_train['SUBJ_APPR_ID'])]\n",
    "#         print('Subset of n = ' + str(sub_sample) + ' loaded successfully!')\n",
    "#     else:\n",
    "#         a_temp =  a_df[a_df['STATE']==state]\n",
    "#         a_train = a_temp.sample(n=sub_sample)\n",
    "#         c_train = c_df[c_df['SUBJ_APPR_ID'].isin(a_train['SUBJ_APPR_ID'])]\n",
    "#         print('Subset of state' + state + ' with n = ' + str(sub_sample) +' loaded successfully!')\n",
    "    \n",
    "#     if a_train.shape[0] *3 != c_train.shape[0]:\n",
    "#         raise RuntimeError(f'Appraisals: {a_train.shape[0]}. Comparables: {c_train.shape[0]}')\n",
    "#     print('')\n",
    "#     print('Number of rows in appraisals dataset: ', a_train.shape[0])\n",
    "#     print('Number of rows in comparables dataset: ', c_train.shape[0])\n",
    "    \n",
    "#     return a_train, c_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eadb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_or_sample_dataset(a_df, c_df, sub_sample=None, state=None):\n",
    "    if sub_sample == None and state == None:\n",
    "        a_train = a_df.copy() # computationally expensive\n",
    "        print('Full dataset loaded successfully!')\n",
    "    elif sub_sample == None and state!=None:\n",
    "        a_train = a_df[a_df['STATE'] == county]\n",
    "        print('Subset from state ' + state + ' loaded successfully!')\n",
    "    elif sub_sample != None and state == None:\n",
    "        a_train = a_df.sample(n=sub_sample)\n",
    "        print('Subset of n = ' + str(sub_sample) + ' loaded successfully!')\n",
    "    else:\n",
    "        a_temp =  a_df[a_df['STATE']==state]\n",
    "        a_train = a_temp.sample(n=sub_sample)\n",
    "        print('Subset of state' + state + ' with n = ' + str(sub_sample) +' loaded successfully!')\n",
    "    \n",
    "    c_train = pd.DataFrame()\n",
    "    for appr_id in a_train['SUBJ_APPR_ID']:\n",
    "        comp_df_temp = c_df[c_df['UNIQUECOMPIDNEW'].isin(dict_apprid_to_uniquecompidnew[appr_id])]\n",
    "        c_train = pd.concat([c_train, comp_df_temp])\n",
    "    \n",
    "    c_train_sample = c_df.sample(n=1000) # choose how many extra comparables to include\n",
    "    c_train = pd.concat([c_train, c_train_sample])\n",
    "    c_train.drop_duplicates(subset=['UNIQUECOMPIDNEW'],keep='first', inplace=True)\n",
    "    \n",
    "    print('Full comparables dataset loaded successfully!')\n",
    "    print('')\n",
    "    print('Number of rows in appraisals dataset: ', a_train.shape[0])\n",
    "    print('Number of rows in unique comparables dataset: ', c_train.shape[0])\n",
    "    \n",
    "    return a_train, c_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "appr_df, comp_df  = full_or_sample_dataset(appr_no_nan, comp_no_nan, sub_sample=2500, state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3556afa",
   "metadata": {},
   "source": [
    "# Feature Importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns categorization:\n",
    "appr_excl_cols = ['COMPNUM', 'ADDRESS1', 'CITY', 'STATE', 'ZIPCODE', 'COUNTY', 'COMPSALEDATE']\n",
    "\n",
    "comp_excl_cols = ['SUBJ_APPR_ID', 'COMPNUM', 'ADDRESS1', 'CITY', 'STATE', 'ZIPCODE', 'COUNTY', 'COMPSALEDATE']\n",
    "\n",
    "calc_cols = ['APPRLATITUDE', 'APPRLONGITUDE', 'SALEDATE']\n",
    "\n",
    "appr_cat_cols =    ['SUBJ_APPR_ID',\n",
    "                    'LOCRTGNEUTRAL', 'LOCRTGBENEFICIAL', 'LOCRTGADVERSE', 'LOCRESIDENTIAL', \n",
    "                    'LOCINDUSTRIAL', 'LOCCOMMERCIAL', 'LOCBUSYROAD', 'LOCWATERFRONT', 'LOCGOLFCOURSE', \n",
    "                    'LOCADJPARK', 'LOCADJPOWERLINE', 'LOCLANDFILL', 'LOCPUBLICTRAN', \n",
    "                    'VIEWRTGNEUTRAL', 'VIEWRTGBENEFICIAL', 'VIEWRTGADVERSE', 'VIEWTYPEWATER', \n",
    "                    'VIEWTYPEPASTORAL', 'VIEWTYPEWOOD', 'VIEWTYPEPARK', 'VIEWTYPEGOLFCOURSE', \n",
    "                    'VIEWTYPECITYSKYLINE', 'VIEWTYPEMOUNTAIN', 'VIEWTYPERESIDENTIAL', 'VIEWTYPECITYSTREET', \n",
    "                    'VIEWTYPEINDUSTRIAL', 'VIEWTYPEPOWERLINE', 'VIEWTYPELIMITED', \n",
    "                    'QUALITYOFCONSTQ1', 'QUALITYOFCONSTQ2', 'QUALITYOFCONSTQ3', 'QUALITYOFCONSTQ4', \n",
    "                    'QUALITYOFCONSTQ5', 'QUALITYOFCONSTQ6', \n",
    "                    'CONDITIONC1', 'CONDITIONC2', 'CONDITIONC3', 'CONDITIONC4', 'CONDITIONC5', 'CONDITIONC6']\n",
    "\n",
    "comp_cat_cols =    ['UNIQUECOMPIDNEW',\n",
    "                    'LOCRTGNEUTRAL', 'LOCRTGBENEFICIAL', 'LOCRTGADVERSE', 'LOCRESIDENTIAL', \n",
    "                    'LOCINDUSTRIAL', 'LOCCOMMERCIAL', 'LOCBUSYROAD', 'LOCWATERFRONT', 'LOCGOLFCOURSE', \n",
    "                    'LOCADJPARK', 'LOCADJPOWERLINE', 'LOCLANDFILL', 'LOCPUBLICTRAN', \n",
    "                    'VIEWRTGNEUTRAL', 'VIEWRTGBENEFICIAL', 'VIEWRTGADVERSE', 'VIEWTYPEWATER', \n",
    "                    'VIEWTYPEPASTORAL', 'VIEWTYPEWOOD', 'VIEWTYPEPARK', 'VIEWTYPEGOLFCOURSE', \n",
    "                    'VIEWTYPECITYSKYLINE', 'VIEWTYPEMOUNTAIN', 'VIEWTYPERESIDENTIAL', 'VIEWTYPECITYSTREET', \n",
    "                    'VIEWTYPEINDUSTRIAL', 'VIEWTYPEPOWERLINE', 'VIEWTYPELIMITED', \n",
    "                    'QUALITYOFCONSTQ1', 'QUALITYOFCONSTQ2', 'QUALITYOFCONSTQ3', 'QUALITYOFCONSTQ4', \n",
    "                    'QUALITYOFCONSTQ5', 'QUALITYOFCONSTQ6', \n",
    "                    'CONDITIONC1', 'CONDITIONC2', 'CONDITIONC3', 'CONDITIONC4', 'CONDITIONC5', 'CONDITIONC6']\n",
    "\n",
    "numerical_cols = ['TOTALRM', 'BDRM', 'BLGRDTOTALSQFT', 'BLGRDFINISHSQFT', 'BLGRDRECRM', \n",
    "                  'BLGRDBEDRM', 'BLGRDOTHERRM', 'GROSSLIVINGAREA', 'ACTUALAGE', 'FULL_BATH', \n",
    "                  'FULL_BLGRDBATHRM', 'HALF_BATH', 'HALF_BLGRDBATHRM', 'SITEAREASQFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c87ca",
   "metadata": {},
   "source": [
    "## *Scale data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF does not need scaling:\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and drop unnecessary columns in comparables dataset:\n",
    "comp_rf_scaled = comp_df.copy()\n",
    "# comp_rf_scaled[numerical_cols] = scaler.fit_transform(comp_rf_scaled[numerical_cols])\n",
    "comp_rf_scaled.drop(comp_excl_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns:\n",
    "appr_rf_scaled = appr_df.copy()\n",
    "# appr_rf_scaled[numerical_cols] = scaler.fit_transform(appr_rf_scaled[numerical_cols])\n",
    "appr_rf_scaled.drop(appr_excl_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "appr_rf_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c3844",
   "metadata": {},
   "source": [
    "## *Helper functions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7108ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(long1, lat1, long2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points\n",
    "    on the earth (specified with latitude and longitude)\n",
    "    Args:\n",
    "        long1:  longitude from appraisal\n",
    "        lat1:   latitude from appraisal\n",
    "        long2:  longitude from comparable\n",
    "        lat2:   latitude from comparable\n",
    "    \"\"\"\n",
    "    R = 6371 # radius km of the earth\n",
    "    long1, lat1, long2, lat2 = map(radians, [long1, lat1, long2, lat2])\n",
    "    dlong = long2 - long1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlong/2)**2\n",
    "    c =  2 * asin(sqrt(a))\n",
    "    dist_km = R * c\n",
    "    \n",
    "    return dist_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879886fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def haversine_distance_quick(long1, lat1, long2, lat2):\n",
    "#     R = 6371\n",
    "#     x = (radians(long2) - radians(long1)) * cos(0.5* (radians(lat2) + radians(lat2)))\n",
    "#     y = radians(lat2) - radians(lat1)\n",
    "#     dist_km = R * sqrt(x*x + y*y)\n",
    "    \n",
    "#     return dist_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def haversine_distance_superquick(long1, lat1, long2, lat2):\n",
    "#     x = lat2 - lat1\n",
    "#     y = (long2 - long1) * cos((lat2 + lat1)* 0.00872664626)\n",
    "#     dist_km = 111.319 * sqrt(x*x + y*y)\n",
    "    \n",
    "#     return dist_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2117430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long1 = float(appr_df.iloc[0]['APPRLONGITUDE'])\n",
    "# lat1 = float(appr_df.iloc[0]['APPRLATITUDE'])\n",
    "\n",
    "# comp_df['DISTTOAPPR'] = comp_df.apply(lambda x: haversine_distance(long1, lat1, float(x['APPRLONGITUDE']), float(x['APPRLATITUDE'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_df['DISTTOAPPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63544e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the test created columns:\n",
    "# comp_df.drop(['DISTTOAPPR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db752cb1",
   "metadata": {},
   "source": [
    "## *Random forest classifier:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75735ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['LOCRTGNEUTRAL', 'LOCRTGBENEFICIAL', 'LOCRTGADVERSE',\n",
    "       'LOCRESIDENTIAL', 'LOCINDUSTRIAL', 'LOCCOMMERCIAL', 'LOCBUSYROAD',\n",
    "       'LOCWATERFRONT', 'LOCGOLFCOURSE', 'LOCADJPARK', 'LOCADJPOWERLINE',\n",
    "       'LOCLANDFILL', 'LOCPUBLICTRAN', 'VIEWRTGNEUTRAL', 'VIEWRTGBENEFICIAL',\n",
    "       'VIEWRTGADVERSE', 'VIEWTYPEWATER', 'VIEWTYPEPASTORAL', 'VIEWTYPEWOOD',\n",
    "       'VIEWTYPEPARK', 'VIEWTYPEGOLFCOURSE', 'VIEWTYPECITYSKYLINE',\n",
    "       'VIEWTYPEMOUNTAIN', 'VIEWTYPERESIDENTIAL', 'VIEWTYPECITYSTREET',\n",
    "       'VIEWTYPEINDUSTRIAL', 'VIEWTYPEPOWERLINE', 'VIEWTYPELIMITED',\n",
    "       'QUALITYOFCONSTQ1', 'QUALITYOFCONSTQ2', 'QUALITYOFCONSTQ3',\n",
    "       'QUALITYOFCONSTQ4', 'QUALITYOFCONSTQ5', 'QUALITYOFCONSTQ6',\n",
    "       'CONDITIONC1', 'CONDITIONC2', 'CONDITIONC3', 'CONDITIONC4',\n",
    "       'CONDITIONC5', 'CONDITIONC6',\n",
    "       'TOTALRM', 'BDRM', 'BLGRDTOTALSQFT', 'BLGRDFINISHSQFT', 'BLGRDRECRM',\n",
    "       'BLGRDBEDRM', 'BLGRDOTHERRM', 'GROSSLIVINGAREA',\n",
    "       'ACTUALAGE', 'FULL_BATH', 'FULL_BLGRDBATHRM', 'HALF_BATH',\n",
    "       'HALF_BLGRDBATHRM', 'SITEAREASQFT', 'DISTTOAPPR', 'TIMEDIFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02819a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_importances = pd.DataFrame(cols, columns=['Features'])\n",
    "count = 0\n",
    "\n",
    "for appr_id in appr_rf_scaled['SUBJ_APPR_ID']:\n",
    "    # Filter the appraisals dataset:\n",
    "    appr_df_temp = appr_rf_scaled.loc[appr_rf_scaled['SUBJ_APPR_ID'] == appr_id]\n",
    "    long1 = appr_df_temp['APPRLONGITUDE']\n",
    "    lat1 = appr_df_temp['APPRLATITUDE']\n",
    "    appr_sale_date = appr_df_temp.loc[appr_df_temp['SUBJ_APPR_ID'] == appr_id,'SALEDATE'].values[0]\n",
    "    \n",
    "    # Work the comparables dataset:\n",
    "    comp_df_temp = comp_rf_scaled.copy()\n",
    "    comp_df_temp['DISTTOAPPR'] = comp_df_temp.apply(lambda x: haversine_distance(long1, lat1, float(x['APPRLONGITUDE']), float(x['APPRLATITUDE'])), axis=1)\n",
    "    comp_df_temp['APPRSALEDATE'] = appr_sale_date\n",
    "    comp_df_temp['TIMEDIFF'] = (appr_sale_date - comp_df_temp['SALEDATE']).dt.days\n",
    "    comp_df_temp['SELECTED'] = 0\n",
    "    comp_df_temp.loc[comp_df_temp['UNIQUECOMPIDNEW'].isin(dict_apprid_to_uniquecompidnew[appr_id]), 'SELECTED'] = 1\n",
    "    \n",
    "    # Filter the comparables dataset:\n",
    "    comp_df_temp2 = comp_df_temp.loc[(comp_df_temp['TIMEDIFF'] >= 0) & (comp_df_temp['DISTTOAPPR'] <= 50)]\n",
    "    \n",
    "    # Prepare training dataset:\n",
    "    y_train = comp_df_temp2['SELECTED']\n",
    "    X_train = comp_df_temp2.drop(['UNIQUECOMPIDNEW', 'SALEDATE', 'APPRSALEDATE', 'SELECTED', 'APPRLATITUDE', 'APPRLONGITUDE'], axis=1)\n",
    "    \n",
    "    # Fit the random forest:\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=1, max_features=0.33)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    data = list(zip(rf.feature_names_in_, rf.feature_importances_))\n",
    "    df_importances = pd.DataFrame(data, columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_importances = df_importances.assign(Ranking=range(len(df_importances)))\n",
    "    df_importances_relevant = df_importances.copy()\n",
    "    col_name = 'Ranking_' + str(count)\n",
    "    df_importances_relevant.rename(columns={\"Ranking\": col_name}, inplace = True)\n",
    "    df_importances_relevant.drop(['Feature','Importance'], axis=1, inplace=True)\n",
    "    \n",
    "    rf_importances = pd.concat([rf_importances, df_importances_relevant], axis=1)\n",
    "    \n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b584795",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_importances[\"TotalRanking\"] = rf_importances.sum(axis=1)\n",
    "rf_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf_df = rf_importances.sort_values(by=['TotalRanking'], ascending=True)\n",
    "final_rf_df[['Features', 'TotalRanking']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f49141",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_rf_df[['Features', 'TotalRanking']].head(30)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
